---
sidebar_position: 4
id: Performance Metrics
title: Performance Metrics
tags:
  - Study
  - Graduate
  - Data Mining
---

# Performance Metrics

## Confusion Matrix for Binary Classification

|                | Predicted Positive | Predicted Negative |
|----------------|-------------------|-------------------|
| Actual Positive | True Positive (TP) | False Negative (FN) |
| Actual Negative | False Positive (FP) | True Negative (TN) |

- Type I Error: False Positive(FP): Mistaken rejection of the null hypothesis (错误拒绝了零假设 -> 以为有关实际无关 --> “误报”)
- Type II Error: False Negative(FN): Mistaken acceptance of the null hypothesis (错误接受了零假设 -> 以为无关实际有关 --> “漏报”)

### Metrics

- True Positive Rate (TPR) / Recall / Sensitivity / Hit Rate / Power: $$TPR = \frac{TP}{TP + FN}$$
- True Negative Rate (TNR) / Specificity: $$TNR = \frac{TN}{TN + FP}$$
- False Positive Rate (FPR) / Fall-out: $$FPR = \frac{FP}{FP + TN}$$
- False Negative Rate (FNR) / Miss Rate: $$FNR = \frac{FN}{FN + TP}$$

此外，还有**真正重要的几个实用指标**：
1. Recall （召回率）: $$Recall = \frac{TP}{TP + FN}$$
2. Precision: $$Precision = \frac{TP}{TP + FP}$$
3. F-score: harmonic mean of Recall and Precision, $$F = \frac{2}{\frac{1}{Recall} + \frac{1}{Precision}} = \frac{2 \cdot Recall \cdot Precision}{Recall + Precision}$$
4. Accuracy （准确率）: $$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$$

### ROC & PR Curve

- ROC Curve: 以FPR为x轴，TPR为y轴的曲线【ROC-AUC越大，模型性能越好】

- PR Curve: 以Recall为x轴，Precision为y轴的曲线【PR-AUC越大，模型性能越好】

`AUC`: Area Under Curve, 计算曲线下的面积 -> 越大，说明两者同步越大，模型性能越好

## Extending to Multi-class Classification

![Confusion Matrix for Multi-class Classification](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/514DM_4.png)

也就是说，对于多分类问题，混淆矩阵的每一行代表了实际类别，而每一列代表了预测类别。  
只有`对角线`是正确预测的样本数。

## Information Theory

### Information Theory 简介

- 高概率时间 = 不令人惊讶 = 低信息量 (eg. 明天太阳会升起)
- 低概率事件 = 令人惊讶 = 高信息量 (eg. 明天贝利亚会带着捷德入侵地球)

**信息量** (Information Content): $$h(x) = -\log_2 Prob(x)$$
其中，负号是为了保证信息量是正数 (毕竟概率是 0~1 )

P.S.  
- CS以2为底，因为计算机是二进制的
- 物理学以e为底，因为自然界的很多现象都是指数增长的
- 工程学、经济学以10为底，因为人类更能直观地理解10进制

### Entropy (信息熵)

- 均匀分布(Uniform Probability Distribution) = 令人惊讶 = 高熵（因为每个事件发生概率均等，最不可预测。eg. 掷骰子）
- 偏斜分布(Skewed Probability Distribution) = 不令人惊讶 = 低熵 （某些事件发生概率远大于其他事件，更容易预测。eg. 明天太阳会正常升起，而不是被后羿射掉）

$ H(X) $ 的定义：
$$H(X) = -\sum_{i=1}^{n} Prob(x_i) \log_2 (Prob(x_i))$$

#### 衡量Entropy的差异

- $ P_(x_i) $: 真实分布
- $ Q_(x_i) $: 预测分布

1. Kullback-Leibler Divergence (KL): $$ D_{KL}(P || Q) = \sum_{i=1}^{n} P(x_i) \log_2 \frac{P(x_i)}{Q(x_i)} $$  
KL 衡量了：如果用 Q 来`代替` P，需要多少`额外`的信息量。

2. Cross-Entropy (交叉熵): $$ H(P, Q) = -\sum_{i=1}^{n} P(x_i) \log_2 (Q(x_i)) $$  
Cross-Entropy 衡量了：如果用 Q 来`表示` P，需要多少`总的`信息量。

#### 交叉熵损失

$$
L = -\sum_{i=1}^{n} log_2 (Predicted_{j, observedClass})
$$
直接举例：

![Cross-Entropy Loss](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/514DM_5.png)

HW5中的例子：  
因为是求和公式，所以当问题为**二分类任务**时，公式就是：
$$
L = -[y_{true} \cdot log_2(\hat{y}) + (1-y_{true}) \cdot log_2(1-\hat{y})]
$$
