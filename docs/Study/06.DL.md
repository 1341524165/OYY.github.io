---
sidebar_position: 6
id: D-Learning
title: 图像处理与深度学习
tags:
  - Study
  - 3A
  - DeepLearning
---

## _图像处理与深度学习_

### 1.基本知识

#### 1.1 机器学习分类

**TEP**（Task 任务，Experience 经验，Performance 性能）

聚类：是指把相似的数据划分到一起，具体划分的时候`并不关心这一类的标签`，目标就是把相似的数据**聚合**到一起，聚类是一种**无监督学习方法；**  
分类：是把不同的数据划分开，其过程是`通过训练`数据集获得一个`分类器`，再通过分类器去预测未知数据，分类是一种**监督学习方法；**  
回归：拟合一条最接近数据变化规律的曲线

卷积的作用：特征提取；  
池化的作用：特征降维；（池化层在两个卷积层间，仅压缩图形尺寸而不丢失信息）

#### 1.2 卷积神经网络

卷积层：对**三维数组**及其**权重**的计算方式。卷积核（参数）在通过逐一滑动窗口计算而得  
池化/采样层：直接抽样选取极小局部的某一元素作为下一层的元素

#### 1.2 循环神经网络（RNN）

引入特性：**时序**处理序列数据（文本上下文）

对序列数据的处理：每次输入序列中的一个单元，然后保存每一个隐层神经元的计算结果，留给下一个输入时该神经元进行使用  
历史信息：保存的隐层单元计算结果，含有上一次输入的信息

#### 1.3 多层感知机（Multi-Layer Perceptron，MLP）

:::note
神经网络**基本单位：神经元**  
:::

多层前馈（全连接）神经网络 => 每层与下层连接；不同层连接；不跨层连接  
“前馈”不代表信号不能回传，而是指网络拓扑结构中不存在环或回路，其中：

输入层：仅接收外界输入，不进行函数处理；  
隐藏层 和 输出层：包含功能神经元，能将接收到的总输入值与一定的阈值进行比较，然后通过**激活函数**处理以产生神经元的输出

p.s. 若将阈值也作为输入信号在神经网络中标出，则除输出层之外，各层会多出一个固定输入为-1 的“哑结点”（dummy node），该节点与下一层的连接权即为阈值

##### 1.3.1. MLP 算法推导

:::info **_过程_**：以`识别手写数字`为例：

假设样本为 n 个手写数字图像，每个数字图像由 28×28 像素组成，每个像素由灰度值表示。  
我们把 28×28 的像素展开变成一个有 784 个维度的一维行向量，即一个样本向量，那么此时 =>  
`输入层`：每个神经元对应一个像素点，共 784 个神经元

因为是识别单个手写数字，其结果会有 0-9 这十种情况，因此 =>  
`输出层`：10 个神经元

`隐藏层`：层数以及单元数则作为要优化的额外超参数，设为 q
:::

p.s. 关于激活函数：  
理想的激活函数是如下图（a)所示的**阶跃函数**，它将输入值映射为输出值“0”或“1”，显然“1”对应于神经元兴奋，“0”对应于神经元抑制；  
然而，阶跃函数具有**不连续、不光滑**等不太好的性质，因此实际常用 **_Sigmoid 函数_**作为激活函数，如下图(b)所示。  
【有时，输出层神经元也采用线性激活函数，即直接以输入值 c 与阈值 θ 的比较结果作为最后的预测值 y’输出。】  
![sigmoid](https://tva2.sinaimg.cn/large/005x6vs8ly1h73ndg7bhtj31350j1di4.jpg)

##### 1.3.2 正向传播(Forward Propagation)：激活神经网络

n 个样本向量组成的矩阵 X 的维度为 n×784，每一行为一个样本向量；这也我们得到输入矩阵：  
![输入数据集矩阵](https://tvax2.sinaimg.cn/large/005x6vs8ly1h73o5g3eqdj33hw150my9.jpg)

而后，隐藏层输入数据：  
![输入层输出；隐藏层输入](https://tvax2.sinaimg.cn/large/005x6vs8ly1h73vhb35e4j32io1dggnw.jpg)

接下来，隐藏层的各个功能神经元将接收到的输入值 ah(h=1,2,…,q)，与某一阈值 γh(h=1,2,…,q)进行比较，  
然后通过激活函数 Sigmoid 处理产生神经元的输出 bh(h=1,2,…,q)，用公式表示为：  
![隐藏层数据处理](https://tvax2.sinaimg.cn/large/005x6vs8ly1h73vlubg3kj30aq01x748.jpg)

于是，隐藏层输出向量为：  
![隐藏层输出](https://tva2.sinaimg.cn/large/005x6vs8ly1h73vt0ydysj30ib01674e.jpg)

之后，隐藏层神经元的输出 bh(h=1,2,…,q)继续通过带权重的连接 w 传递至输出层，成为输出层的输入值 co(o=1,2,…,l)，  
同理，参照上面的 V 矩阵可知此处的 **W 的转置** 将是一个 **q×l** 的矩阵；

同理 Sigmoid 处理：  
![输出层数据处理](https://tva4.sinaimg.cn/large/005x6vs8ly1h73w06px2gj30aq01xaa0.jpg)

后得到：  
![输出层输出](https://tva2.sinaimg.cn/large/005x6vs8ly1h73w0qi42jj30in014jrh.jpg)

##### 1.3.3 逆向传播(Back Propagation)：学习权重系数及阈值

:::danger
对不起！后续找机会补上，麻瓜大脑看点矩阵的简单运算都花了不少时间/(ㄒ o ㄒ)/~~😥  
今后一定更加好好努力！！
:::
但至少应该学习一个最基本却应用十分广泛的的方法：**_梯度下降法(Gradient Descent)_**；  
梯度下降可以说是神经网络最常用的优化算法，其原理：

```markdown
目标函数 J(θ)关于参数 θ 的梯度将是损失函数（loss function）上升最快的方向。  
而我们要最小化 loss，只需要将参数沿着梯度相反的方向前进一个步长，就可以实现目标函数的下降。  
这个步长 η 又称为学习速率。
```

更新参数的公式为：
**θ = θ - η \* ∇J(θ)**

:::tip 来个有点不通俗易懂的通俗易懂的例子
损失函数上升的方向是坏的，把他当成比较累的上山；  
也就是说梯度就是上山；  
所以我们要最小化损失函数，也就是要往下山的方向前进步长；  
那么下山的速度也就是步长、学习速率 η。
:::

回到数学上来，比如 t 时刻的观测值为 x(t) ，那么评估 t 时刻的**移动平均值**为:  
v(t) = η \* x(t) + (1 - η) \* v(t-1) `η 取 0~1`

递推展开后可知，x 的权重是在**指数递减**的，也就是说，**最近的观测值对 v 的影响最大**；
这就要提到这一张图：  
![image](https://tva4.sinaimg.cn/large/005x6vs8ly1h741djufmqj30pw04n403.jpg)
显然，最近的时候由于其波动幅度较大，梯度下降速度被减慢；η 又不宜过大避免偏离函数范围  
这展现了`加权移动平均`的重要性：**减少噪声**，**减少波动**。  
于是我们得到一个`平滑且优化速率高`的梯度下降算法。
