---
sidebar_position: 3
id: CNN
title: 3. CNN卷积神经网络
tags:
  - Study
  - 3A
  - DeepLearning
---

## _CNN 卷积神经网络_

在 MLP 中我们可以发现所举的例子是 **28\*28 尺寸**的图像，那么当图像尺寸变大到 **96\*96** 时，如果我们继续全连接，那么参数量庞大的问题将导致 => **易过拟合 + 效率低下**

于是我们得到了 CNN，它的特点是：**局部连接 + 共享权重**，这样就可以大大减少参数量，从而提高效率。

### 1. 卷积结构

:::info
与常规神经网络不同，卷积神经网络的各层中的神经元是 3 维排列的：宽度、高度和深度。  
其中的宽度和高度是很好理解的，因为本身卷积就是一个二维模板，但是在卷积神经网络中的**深度指的是激活数据体的第三个维度**，而不是整个网络的深度，整个网络的深度指的是网络的层数。  
举个例子来理解什么是宽度，高度和深度，假如使用 CIFAR-10 中的图像是作为卷积神经网络的输入，该输入数据体的维度是 32x32x3（宽度，高度和深度）。  
我们将看到，层中的**神经元将只与`前一层中的一小块区域`连接**，而不是采取全连接方式。对于用来分类 CIFAR-10 中的图像的卷积网络，其最后的输出层的维度是 1x1x10，因为在卷积神经网络结构的最后部分将会把全尺寸的图像**压缩为**包含分类评分的**一个向量**，向量是在**深度方向排列**的。  
——来自[这篇文章](https://zhuanlan.zhihu.com/p/47184529)
:::

下面是例子：  
![卷积结构](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/DL_juanjijiegou.jpg)  
左：传统 3 层全连接； 右：CNN  
卷积神经网络的每一层都将 3D 的输入数据变化为神经元 3D 的激活数据并输出。  
红色的输入层代表输入图像，所以它的宽度和高度就是图像的宽度和高度，它的**深度是 3**（代表了红、绿、蓝 3 种颜色通道），与红色相邻的蓝色部分是**经过卷积和池化之后的激活值（也可以看做是神经元）** ，后面是接着的卷积池化层。

### 2. layers

卷积层（Convolutional layer）:  
卷积神经网路中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过反向传播算法优化得到的。卷积运算的目的是**提取输入的不同特征**，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。

线性整流层（Rectified Linear Units layer, `ReLU layer`）:  
这一层神经的活性化函数（Activation function）使用线性整流（Rectified Linear Units, ReLU）f(x)=max(0,x)。

池化层（Pooling layer）:  
通常在卷积层之后会得到维度很大的特征，将特征切成几个区域，取其最大值或平均值，得到新的、维度较小的特征。

全连接层（ Fully-Connected layer）,:  
把所有**局部特征结合变成全局特征**，用来计算最后每一类的得分。

#### 2.1 卷积层

每个隐含单元连接的输入区域大小叫 r 神经元的`感受野(receptive field)`。
